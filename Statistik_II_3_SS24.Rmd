---
title: "Einheit 3"
subtitle: "‚öî<br/>with xaringan"
author: "Prof. Dr. Stephan Goerigk"
institute: "RStudio, PBC"
date: "2016/12/12 (updated: `r Sys.Date()`)"
output:
  xaringan::moon_reader:
    css: [xaringan-themer.css, "hygge", style.css]
    lib_dir: libs
    nature:
      slideNumberFormat: "%current%"
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: '16:9'
    seal: false
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)

library(tidyverse)
library(kableExtra)
library(ggplot2)
library(plotly)
library(htmlwidgets)
library(plotly)
library(MASS)
library(ggpubr)
library(xaringanthemer)
library(xaringanExtra)

style_duo_accent(
  primary_color = "#621C37",
  secondary_color = "#EE0071",
  background_image = "blank.png"
)

xaringanExtra::use_xaringan_extra(c("tile_view"))

use_scribble(
  pen_color = "#EE0071",
  pen_size = 4
  )

knitr::opts_chunk$set(
  fig.retina = TRUE,
  warning = FALSE,
  message = FALSE
)

Xname = ""
Yname = ""
nudgnumber = 3
my_green = "#EE0071"
```

name: Title slide
class: middle, left
<br><br><br><br><br><br><br>
# Statistik II
***
### Einheit 3: Einfache lineare Regression (1)
##### `r format(as.Date(data.frame(readxl::read_excel("Modul Quantitative Methoden II_Termine.xlsx"))$Datum), "%d.%m.%Y")[3]` | Prof. Dr. Stephan Goerigk

---
class: top, left
### Einfache lineare Regression

#### Zusammenh√§nge - Korrelation vs. Regression

.center[
```{r eval = TRUE, echo = F, out.width = "950px"}
knitr::include_graphics("bilder/f1.png")
```
]

```{r message=FALSE, warning=FALSE, include=FALSE}
set.seed(13)

ex = data.frame(X = 1:100)
ex$Y = ex$X*3 + rnorm(100,0,25)

ggplot(data = ex, aes(X, Y)) +
  geom_point() +
  geom_smooth(method = "lm", se = F, colour = my_green) +
  theme_classic() +
  theme(axis.text = element_blank(), axis.ticks = element_blank())

ex2 = data.frame(X = 1:100)
ex2$Y = ex2$X + rnorm(100,0,10000000)

ggplot(data = ex2, aes(X, Y)) +
  geom_point() +
  geom_smooth(method = "lm", se = F, colour = my_green) +
  theme_classic() +
  theme(axis.text = element_blank(), axis.ticks = element_blank())
```

```{r include=FALSE}
sampledata = read.csv("sampledata.csv")
model = lm(Y ~ X, data = sampledata)
sampledata$pred = predict(model, newdata = sampledata)
sampledata_long = multilevel::make.univ(sampledata, sampledata[,3:4], outname = "Y")
sampledata_long = sampledata_long[,c(1,2, 4, 5,6)]
names(sampledata_long) = c("ID", "X", "pred", "Predicted", "Y")
sampledata_long$Predicted = factor(sampledata_long$Predicted, levels = 0:1, labels = c("yi Beobachtungswert", "≈∑i gesch√§tzter Wert"))
sampledata_long$ID[sampledata_long$Predicted ==  "≈∑i gesch√§tzter Wert"] = ""
```

---
class: top, left
### Einfache lineare Regression

#### Zusammenh√§nge - Korrelation vs. Regression

**Ungerichtete Zusammenh√§nge**

* Wir haben uns bereits mit ungerichteten Zusammenh√§ngen zwischen 2 Variablen besch√§ftigt

* ungerichtet: es k√∂nnte $X$ auf $Y$ wirken, $Y$ aber auch auf $X$, oder die beiden k√∂nnten einfach parallel auftreten 

*  Um solche Zusammenh√§nge zu messen, gibt es sogenannte Assoziationsma√üe z.B. Varianz oder Korrelation.

**Gerichtete Zusammenh√§nge**

* Jetzt wollen wir einen Schritt weiter gehen und unterstellen, dass der Zusammenhang eine Wirkrichtung hat

* D.h. $X$ wirkt auf $Y$ und eben nicht $Y$ wirkt auf $X$ 

* Wenn wir diese Richtung unterstellen, kann man die lineare Regression anwenden

---
class: top, left
### Einfache lineare Regression

#### Zusammenh√§nge - Korrelation vs. Regression

* Die Schreibweise $X \rightarrow Y$ soll also andeuten $X$ wirkt auf $Y$ (bzw. sagt $Y$ vorher) 

* Dabei ist $X$ die unabh√§ngige Variable (UV) und $Y$ ist die abh√§ngige Variable (AV)

Beispiel: 

* Die Intensit√§t des Sports wirkt sich auf den Puls aus, aber nicht umgekehrt

* Wenn wir so einen gerichteten Zusammenhang untersuchen, dann stellt sich die Frage, wo die Richtung herkommt

$\rightarrow$ Typischerweise m√ºssen wir die Richtung aus der Theorie oder aus Plausibilit√§ts√ºberlegungen herleiten 

---
class: top, left
### Einfache lineare Regression

#### Einfache lineare Regression, Beispiel

Gegeben: Lernaufwand X (Stunden) und Klausurerfolg Y (Punkte: 0-100) von n = 26 Studierenden (hier Zeilen 1-13).

.pull-left[
```{r echo = F, out.height = "420px"}
scatterplot = ggplot(data = sampledata, aes(x = X, y = Y, label = ID)) +
  geom_point() +
  geom_text(check_overlap = TRUE, vjust = 0, nudge_y = nudgnumber) +
   theme_classic() +
  labs(x = "Lernaufwand (X)", y = "Klausurerfolg (Y)") +
  theme(rect = element_rect(fill = "transparent")) +
   coord_cartesian(xlim = c(0,100), ylim = c(0,100))
ggplotly(scatterplot)
```
]
.pull-right[

```{r echo=FALSE}
knitr::kable((sampledata[1:13,1:3]),
             booktabs = T,
             longtable = F) %>%
  kable_classic(full_width = T,
                  font_size = 12,
                  html_font = "Times New Roman")
```
]

---
class: top, left
### Einfache lineare Regression

#### Lineare Regressionsfunktion

$X \rightarrow Y$ Regressionsfunktion und Beobachtungswerte

.pull-left[
```{r echo = F, out.height = "420px"}
scatterplot = ggplot(data = sampledata_long[sampledata_long$Predicted == "yi Beobachtungswert",], aes(x = X, y = Y, label = ID, colour = Predicted)) +
  geom_point() +
  #geom_text(check_overlap = TRUE, vjust = 0, nudge_y = nudgnumber) +
  scale_color_manual(values = c("yi Beobachtungswert" = "black",
                                  "≈∑i gesch√§tzter Wert" = my_green)) +
   theme_classic() +
 coord_cartesian(xlim = c(0,100), ylim = c(0,100)) +
   labs(colour = "", x = "Lernaufwand (X)", y = "Klausurerfolg (Y)") +
theme(rect = element_rect(fill = "transparent")) 
ggplotly(scatterplot) %>%
layout(legend = list(x = 100, y = 0.5))
```
]

.pull-right[
* Jeder Punkt repr√§sentiert eine Kombination aus $X$ und $Y$ Werten 

* Wir k√∂nnten also sagen, jeder Punkt ist eine Person aus unserem Beispiel 

* Es gilt, in der Regression eine Funktion zu finden, die diese Daten m√∂glichst genau widerspiegelt 
]

---
class: top, left
### Einfache lineare Regression

#### Lineare Regressionsfunktion

$X \rightarrow Y$ Regressionsfunktion und Beobachtungswerte

.pull-left[
```{r echo = F, out.height = "420px"}
scatterplot = ggplot(data = sampledata_long[sampledata_long$Predicted == "yi Beobachtungswert",], aes(x = X, y = Y, label = ID, colour = Predicted)) +
  geom_point() +
  #geom_text(check_overlap = TRUE, vjust = 0, nudge_y = nudgnumber) +
  scale_color_manual(values = c("yi Beobachtungswert" = "black",
                                  "≈∑i gesch√§tzter Wert" = my_green)) +
  geom_abline(slope = coef(model)[["X"]], 
              intercept = coef(model)[["(Intercept)"]], colour = my_green) +
   theme_classic() +
 coord_cartesian(xlim = c(0,100), ylim = c(0,100)) +
  labs(colour = "", x = "Lernaufwand (X)", y = "Klausurerfolg (Y)") +
theme(rect = element_rect(fill = "transparent")) 
ggplotly(scatterplot) %>%
layout(legend = list(x = 100, y = 0.5))
```
]

.pull-right[
* Im Falle der *linearen* Regression wird unterstellt, dass diese Funktion linear, also eine Gerade ist
]

---
class: top, left
### Einfache lineare Regression

#### Lineare Regressionsfunktion

$X \rightarrow Y$ Regressionsfunktion und Beobachtungswerte

.pull-left[
```{r echo = F, out.height = "420px"}
scatterplot = ggplot(data = sampledata_long[sampledata_long$Predicted == "yi Beobachtungswert",], aes(x = X, y = Y, label = ID, colour = Predicted)) +
  geom_point() +
  #geom_text(check_overlap = TRUE, vjust = 0, nudge_y = nudgnumber) +
  scale_color_manual(values = c("yi Beobachtungswert" = "black",
                                  "≈∑i gesch√§tzter Wert" = my_green)) +
  geom_smooth(se = F, colour = my_green)+
   theme_classic() +
 coord_cartesian(xlim = c(0,100), ylim = c(0,100)) +
  labs(colour = "", x = "Lernaufwand (X)", y = "Klausurerfolg (Y)") +
theme(rect = element_rect(fill = "transparent")) 
ggplotly(scatterplot) %>%
layout(legend = list(x = 100, y = 0.5))
```
]

.pull-right[
* Theoretisch w√§ren allerdings auch andere Funktionen denkbar.

* Diese beschreiben die vorliegenden Daten ggf. besser, sind aber nicht so leicht interpretierbar/generalisierbar.
]

---
class: top, left
### Einfache lineare Regression

#### Lineare Regressionsfunktion

$X \rightarrow Y$ Regressionsfunktion und Beobachtungswerte

.pull-left[
```{r echo = F, out.height = "420px"}
scatterplot = ggplot(data = sampledata_long[sampledata_long$Predicted == "yi Beobachtungswert",], aes(x = X, y = Y, label = ID, colour = Predicted)) +
  geom_segment(aes(x = 0, 
                   xend = 12,
                   y=23,
                   yend= 23), linetype = "dotted", colour = "grey") +
  geom_segment(aes(x = 12, 
                   xend = 12,
                   y=0,
                   yend= 23), linetype = "dotted", colour = "grey") +
  geom_point() +
 # geom_text(check_overlap = TRUE, vjust = 0, nudge_y = nudgnumber) +
  scale_color_manual(values = c("yi Beobachtungswert" = "black",
                                  "≈∑i gesch√§tzter Wert" = my_green)) +
  geom_abline(slope = coef(model)[["X"]], 
              intercept = coef(model)[["(Intercept)"]], colour = my_green) +
  annotate(geom = "Text",  label = "x<sub>i</sub>", x = 12/2, y = 23+nudgnumber+2) +
  annotate(geom = "Text",  label = "y<sub>i</sub>", x = 12+nudgnumber+2, y = 23/2) +
   theme_classic() +
  labs(colour = "", x = "Lernaufwand (X)", y = "Klausurerfolg (Y)") +
  theme(rect = element_rect(fill = "transparent")) +
 coord_cartesian(xlim = c(0,100), ylim = c(0,100))
ggplotly(scatterplot) %>%
layout(legend = list(x = 100, y = 0.5))
```
]

.pull-right[
* Jeder Beobachtungspunkt hat f√ºr den $X$ Wert einen entsprechenden $Y$ Wert.

* Er ist somit eindeutig f√ºr die beiden Variablen definiert.

ABER:

* F√ºr jeden gegebenen $X$ Wert l√§sst sich ein Punkt auf der Geraden finden, der einen anderen $Y$ Wert hat
]
---
class: top, left
### Einfache lineare Regression

#### Lineare Regressionsfunktion

$X \rightarrow Y$ Regressionsfunktion und Beobachtungswerte

.pull-left[
```{r echo = F, out.height = "420px"}
scatterplot = ggplot(data = sampledata_long, aes(x = X, y = Y, label = ID, colour = Predicted)) +
   geom_segment(aes(x = 0, 
                   xend = 12,
                   y=23,
                   yend= 23), linetype = "dotted", colour = "grey") +
  geom_segment(aes(x = 12, 
                   xend = 12,
                   y=0,
                   yend= 23), linetype = "dotted", colour = "grey") +
  geom_segment(aes(x = 0,
                   xend = 12,
                   y=40.42,
                   yend= 40.42), linetype = "dotted", colour = "grey") +
  geom_segment(aes(x = 12,
                   xend = 12,
                   y=0,
                   yend= 40.42), linetype = "dotted", colour = "grey") +
  geom_point() +
 # geom_text(check_overlap = TRUE, vjust = 0, nudge_y = nudgnumber) +
  scale_color_manual(values = c("yi Beobachtungswert" = "black",
                                  "≈∑i gesch√§tzter Wert" = my_green)) +
  geom_abline(slope = coef(model)[["X"]], 
              intercept = coef(model)[["(Intercept)"]], colour = my_green) +
  annotate(geom = "Text",  label = "x<sub>i</sub>", x = 12/2, y = 23+nudgnumber+2) +
  annotate(geom = "Text",  label = "y<sub>i</sub>", x = 12+nudgnumber+2, y = 23/2) +
  annotate(geom = "Text",  label = "x<sub>i</sub>", x = 12/2, y = sampledata_long$Y[sampledata_long$X == 12 & sampledata_long$Predicted == "≈∑i gesch√§tzter Wert"]+nudgnumber+2) +
  annotate(geom = "Text",  label = "≈∑<sub>i</sub>", x = 12+nudgnumber+2, y = sampledata_long$Y[sampledata_long$X == 12 & sampledata_long$Predicted == "≈∑i gesch√§tzter Wert"]/1.333) +
   theme_classic() +
   labs(colour = "", x = "Lernaufwand (X)", y = "Klausurerfolg (Y)") +
  theme(rect = element_rect(fill = "transparent")) +
 coord_cartesian(xlim = c(0,100), ylim = c(0,100))
ggplotly(scatterplot) %>%
layout(legend = list(x = 100, y = 0.5))
```
]

.pull-right[
* Der pinke Punkt ist der gem√§√ü der linearen Funktion gesch√§tzte $Y$ Wert f√ºr den Punkt X 

* Es ist also der Wert, den man unter Annahme eines linearen Zusammenhangs **erwarten** w√ºrde

* Diese Punkte haben den $X$ Wert gemeinsam aber sind unterschiedlich im $Y$ Wert.
]

---
class: top, left
### Einfache lineare Regression

#### Lineare Regressionsfunktion

$X \rightarrow Y$ Regressionsfunktion und Beobachtungswerte

.pull-left[
```{r echo = F, out.height = "400px"}
scatterplot = ggplot(data = sampledata_long, aes(x = X, y = Y, label = ID, colour = Predicted)) +
   geom_segment(aes(x = 0, 
                   xend = 12,
                   y=23,
                   yend= 23), linetype = "dotted", colour = "grey") +
  geom_segment(aes(x = 12, 
                   xend = 12,
                   y=0,
                   yend= 23), linetype = "dotted", colour = "grey") +
  geom_segment(aes(x = 0, 
                   xend = 12,
                   y=40.43,
                   yend= 40.43), linetype = "dotted", colour = "grey") +
  geom_segment(aes(x = 12, 
                   xend = 12,
                   y=0,
                   yend= 40.43), linetype = "dotted", colour = "grey") +
  geom_segment(aes(x = 12, 
                   xend = 12,
                   y=23,
                   yend= 40.43), linetype = "dotted", colour = "red") +
  geom_point() +
  #geom_text(check_overlap = TRUE, vjust = 0, nudge_y = nudgnumber) +
  scale_color_manual(values = c("yi Beobachtungswert" = "black",
                                  "≈∑i gesch√§tzter Wert" = my_green)) +
  geom_abline(slope = coef(model)[["X"]], 
              intercept = coef(model)[["(Intercept)"]], colour = my_green) +
  annotate(geom = "Text",  label = "x<sub>i</sub>", x = 12/2, y = 23+nudgnumber+2) +
  annotate(geom = "Text",  label = "y<sub>i</sub>", x = 12+nudgnumber+2, y = 23/2) +
  annotate(geom = "Text",  label = "x<sub>i</sub>", x = 12/2, y = sampledata_long$Y[sampledata_long$X == 12 & sampledata_long$Predicted == "≈∑i gesch√§tzter Wert"]+nudgnumber+2) +
  annotate(geom = "Text",  label = "≈∑<sub>i</sub>", x = 12+nudgnumber+2, y = sampledata_long$Y[sampledata_long$X == 12 & sampledata_long$Predicted == "≈∑i gesch√§tzter Wert"]/1.333) +
   theme_classic() +
   labs(colour = "", x = "Lernaufwand (X)", y = "Klausurerfolg (Y)") +
  theme(rect = element_rect(fill = "transparent")) +
 coord_cartesian(xlim = c(0,100), ylim = c(0,100))
ggplotly(scatterplot) %>%
layout(legend = list(x = 100, y = 0.5))
```
]

.pull-right[
* Wie wir aber sehen, gibt es hier einen Unterschied in den beiden $Y$ Werten 

* Dieser Unterschied ist unser sogenannter Vorhersagefehler oder auch **Residuum** 
  * Differenz zwischen Beobachtungswert und vorhergesagtem Wert
  * Das Residuum wird mit $\varepsilon_i$ bezeichnet

Formel f√ºr das Residuum:

$$\varepsilon_i=\hat{y}_i - y_i$$
]


---
class: top, left
### Einfache lineare Regression

#### Lineare Regressionsfunktion

$X \rightarrow Y$ Regressionsfunktion und Beobachtungswerte

.pull-left[
```{r echo = F, out.height = "420px"}
scatterplot = ggplot(data = sampledata_long, aes(x = X, y = Y, label = ID, colour = Predicted)) +
  geom_segment(x = 0, xend = 10, y = coef(model)[["(Intercept)"]], yend = coef(model)[["(Intercept)"]], colour = "black") +
  geom_segment(x = 10, xend = 10, y = coef(model)[["(Intercept)"]], yend = predict(model, newdata = data.frame(X=10)), colour = "black") +
  geom_point(x = 0, y = coef(model)[["(Intercept)"]], colour = my_green) +
  geom_point() +
  geom_abline(slope = coef(model)[["X"]], 
              intercept = coef(model)[["(Intercept)"]], colour = my_green) +
   theme_classic()+
  scale_color_manual(values = c("yi Beobachtungswert" = "black",
                                  "≈∑i gesch√§tzter Wert" = my_green)) +
  theme(rect = element_rect(fill = "transparent")) +
  labs(colour = "", x = "Lernaufwand (X)", y = "Klausurerfolg (Y)") +
  annotate(geom = "Text",  label = "a", x = 0, y = coef(model)[["(Intercept)"]] + 5, colour =  "red") +
  annotate(geom = "Text",  label = "b", x = 12, y = predict(model, newdata = data.frame(X=10)) - (predict(model, newdata = data.frame(X=10)) - coef(model)[["(Intercept)"]])/2, colour =  "red") +
 coord_cartesian(xlim = c(0,100), ylim = c(0,100)) #+
#theme(axis.text = element_blank(), axis.ticks = element_blank(), axis.line = element_blank(), axis.title = element_blank())
ggplotly(scatterplot) %>%
layout(legend = list(x = 100, y = 0.5))
```
]

.pull-right[
$$\hat{y}_i=a+b \cdot x_i + \varepsilon_i$$
$a:$ Y-Achsenabschnitt <br>
$b:$ Steigungsparameter

**Interpretation:**

$a:$ Wert, den $Y$ hat, wenn $X=0$ ist <br>
$b:$ Ver√§nderung von Y bei Zunahme von $X$ um 1 Einheit
]

---
class: top, left
### Einfache lineare Regression

#### Residuen und Zielfunktion

.pull-left[
```{r echo = F, out.height = "420px"}
scatterplot = ggplot(data = sampledata_long[sampledata_long$Predicted == "yi Beobachtungswert",], aes(x = X, y = Y, label = ID, colour = Predicted)) +
  geom_point() +
  geom_abline(slope = coef(model)[["X"]] - coef(model)[["X"]]*0.8, 
              intercept = coef(model)[["(Intercept)"]], colour = my_green, linetype = "dashed") +
  geom_abline(slope = coef(model)[["X"]]- coef(model)[["X"]]*0.5, 
              intercept = coef(model)[["(Intercept)"]], colour = my_green, linetype = "dashed") +
  geom_abline(slope = coef(model)[["X"]], 
              intercept = coef(model)[["(Intercept)"]], colour = my_green) +
  geom_abline(slope = coef(model)[["X"]]+ coef(model)[["X"]]*0.5, 
              intercept = coef(model)[["(Intercept)"]], colour = my_green, linetype = "dashed") +
  geom_abline(slope = coef(model)[["X"]] + coef(model)[["X"]]*0.8, 
              intercept = coef(model)[["(Intercept)"]], colour = my_green, linetype = "dashed") +
   theme_classic()+
   labs(colour = "", x = "Lernaufwand (X)", y = "Klausurerfolg (Y)") +
  scale_color_manual(values = c("yi Beobachtungswert" = "black",
                                  "≈∑i gesch√§tzter Wert" = my_green)) +
  theme(rect = element_rect(fill = "transparent")) +
 coord_cartesian(xlim = c(0,100), ylim = c(0,100))
ggplotly(scatterplot)%>%
layout(legend = list(x = 100, y = 0.5))
```
]

.pull-right[
$$\hat{y}_i=a+b \cdot x_i + \varepsilon_i$$
$a:$ Y-Achsenabschnitt <br>
$b:$ Steigungsparameter

* Theoretisch sind endlos viele Geraden denkbar, die die Punktewolke alle an unterschiedlichen Stellen durchschneiden

* Wir wollen aber genau die Gerade finden, welche die Daten am allerbesten beschreibt.
]

---
class: top, left
### Einfache lineare Regression

#### Residuen und Zielfunktion

.pull-left[
```{r echo = F, out.height = "420px"}
scatterplot = ggplot(data = sampledata_long, aes(x = X, y = Y, colour = Predicted)) +
  geom_segment(aes(xend = X, yend = pred), linetype = "dotted", colour = "red") +
  geom_point() +
  geom_abline(slope = coef(model)[["X"]], 
              intercept = coef(model)[["(Intercept)"]], colour = my_green) +
   theme_classic() +
  theme(rect = element_rect(fill = "transparent")) +
  scale_color_manual(values = c("yi Beobachtungswert" = "black",
                                  "≈∑i gesch√§tzter Wert" = my_green)) +
    labs(colour = "", x = "Lernaufwand (X)", y = "Klausurerfolg (Y)") +
 coord_cartesian(xlim = c(0,100), ylim = c(0,100))
ggplotly(scatterplot) %>%
layout(legend = list(x = 100, y = 0.5))
```
]

.pull-right[
**Ziel:**

* Y-Achsenabschnitt und Steigung so w√§hlen, dass die lineare Funktion die Punkte m√∂glichst gut widerspiegelt 

* gut widerspiegeln = Abstand zwischen dem Beobachtungswert und dem gem√§√ü linearer Funktion gesch√§tzten Wert m√∂glichst klein halten

**Bildliche Vorstellung: **

Wenn ich die Residuen aller Beobachtungswerte zu einer Schnur aneinanderh√§nge, soll diese Schnur m√∂glichst kurz sein
]


---
class: top, left
### Einfache lineare Regression

#### Residuen und Zielfunktion

.pull-left[
```{r echo = F, out.height = "420px"}
scatterplot = ggplot(data = sampledata_long, aes(x = X, y = Y, colour = Predicted)) +
  geom_segment(aes(xend = X, yend = pred), linetype = "dotted", colour = "red") +
  geom_point() +
  geom_abline(slope = coef(model)[["X"]], 
              intercept = coef(model)[["(Intercept)"]], colour = my_green) +
   theme_classic() +
  theme(rect = element_rect(fill = "transparent")) +
  scale_color_manual(values = c("yi Beobachtungswert" = "black",
                                  "≈∑i gesch√§tzter Wert" = my_green)) +
    labs(colour = "", x = "Lernaufwand (X)", y = "Klausurerfolg (Y)") +
 coord_cartesian(xlim = c(0,100), ylim = c(0,100))
ggplotly(scatterplot) %>%
layout(legend = list(x = 100, y = 0.5))
```
]

.pull-right[
Es liegt ein Optimierungsproblem vor:

* Die Summe der quadrierten Residuen wird √ºber alle Beobachtungswerte minimiert 

* So werden die optimalen Werte f√ºr a und b gefunden

* Quadrierung verhindert, dass sich negative und positive Werte ausgleichen

$$\sum\limits _{i=1}^{n}\varepsilon^2_i=\varepsilon^2_1+\varepsilon^2_2...+\varepsilon^2_n \rightarrow \min_{a,b}$$
]

---
class: top, left
### Einfache lineare Regression

#### Bestimmung der zu sch√§tzenden Parameter

* Sch√§tzung von a und b $\rightarrow$ **Methode der kleinsten Quadrate**
 
* Ziel: Summe der quadrierten Residuen minimieren

.pull-left[

**Analytische L√∂sung des Optimierungsproblems:**

1. Y-Achsenabschnitt $(a)$

$$a=\bar{y} - b \cdot \bar{x}$$

2. Y-Steigungsparameter $(b)$

$$b=\frac{\sigma^2_{XY}}{\sigma^2_X} = r_{XY} \cdot \frac{\sigma_y}{\sigma_x}$$
]

---
class: top, left
### Einfache lineare Regression

#### Bestimmung der zu sch√§tzenden Parameter

* Sch√§tzung von a und b $\rightarrow$ **Methode der kleinsten Quadrate**
 
* Ziel: Summe der quadrierten Residuen minimieren

.pull-left[

**Analytische L√∂sung des Optimierungsproblems:**

1. Y-Achsenabschnitt $(a)$

$$a=\bar{y} - b \cdot \bar{x}$$

2. Y-Steigungsparameter $(b)$

$$b=\frac{\sigma^2_{XY}}{\sigma^2_X} = r_{XY} \cdot \frac{\sigma_y}{\sigma_x}$$
]
.pull-right[
**To Do - wir ben√∂tigen:**
* Mittelwert von $X: \bar{x}$ 
* Mittelwert von $Y: \bar{y}$ 
* Kovarianz von $XY: \sigma^2_{XY}$
* Varianz von $X: \sigma^2_X$

]

---
class: top, left
### Einfache lineare Regression

#### Bestimmung der zu sch√§tzenden Parameter

```{r echo = F}
knitr::kable((t(sampledata[,2:3])),
             booktabs = T,
             longtable = F) %>%
  kable_classic(full_width = T,
                  font_size = 18,
                  html_font = "Times New Roman")
```

.pull-left[

<br>
<br>
Mittelwert von $X = \bar{x}=\frac{\sum\limits _{i=1}^{n}x_{i}}{n}$  
<br>
<br>
Mittelwert von $Y = \bar{y}=\frac{\sum\limits _{i=1}^{n}y_{i}}{n}$  

]

.pull-right[

<br>
<br>
Kovarianz von $XY = \sigma_{xy}^2=\frac{\displaystyle \sum_{i=1}^{n}(x_{i}-\bar{x})\cdot(y_{i}-\bar{y})}{n-1}$ 
<br>
<br>
Varianz von $X =  \sigma^2=\dfrac{\displaystyle\sum_{i=1}^{n}(x_{i}-\bar{x})^2}{n-1}$ 

]


---
class: top, left
### Einfache lineare Regression

#### Bestimmung der zu sch√§tzenden Parameter

```{r echo = F}
knitr::kable((t(sampledata[,2:3])),
             booktabs = T,
             longtable = F) %>%
  kable_classic(full_width = T,
                  font_size = 18,
                  html_font = "Times New Roman")
```

.pull-left[

<br>
<br>
Mittelwert von $X = \bar{x}=\frac{\sum\limits _{i=1}^{n}x_{i}}{n}$  = `r round(mean(sampledata$X, na.rm = T),2)`
<br>
<br>
Mittelwert von $Y = \bar{y}=\frac{\sum\limits _{i=1}^{n}y_{i}}{n}$  =  `r round(mean(sampledata$Y, na.rm = T),2)`

]

.pull-right[

<br>
<br>
Kovarianz von $XY = \sigma_{xy}^2=\frac{\displaystyle \sum_{i=1}^{n}(x_{i}-\bar{x})\cdot(y_{i}-\bar{y})}{n-1}$ = `r round(cov(sampledata$X, sampledata$Y), 2)`
<br>
<br>
Varianz von $X =  \sigma^2=\dfrac{\displaystyle\sum_{i=1}^{n}(x_{i}-\bar{x})^2}{n-1}$ =  `r round(var(sampledata$X, na.rm = T),2)`

]

---
class: top, left
### Einfache lineare Regression

#### Bestimmung der zu sch√§tzenden Parameter

```{r echo = F}
knitr::kable((t(sampledata[,2:3])),
             booktabs = T,
             longtable = F) %>%
  kable_classic(full_width = T,
                  font_size = 18,
                  html_font = "Times New Roman")
```


$$ b=\frac{\sigma_{xy}^2}{\sigma^2_x}=\frac{`r round(cov(sampledata$X, sampledata$Y), 2)`}{`r round(var(sampledata$X, na.rm = T),2)`} = `r round(cov(sampledata$X, sampledata$Y) / var(sampledata$X, na.rm = T), 2)`$$

<br>

$$ a = \bar{y} - b \cdot \bar{x} = `r round(mean(sampledata$Y, na.rm = T) - cov(sampledata$X, sampledata$Y) / var(sampledata$X, na.rm = T) * mean(sampledata$X, na.rm = T),2)`$$

<br>

$$ y=a + b\cdot x$$

<br>

<div class="red">
$$ y=`r round(mean(sampledata$Y, na.rm = T) - cov(sampledata$X, sampledata$Y) / var(sampledata$X, na.rm = T) * mean(sampledata$X, na.rm = T),2)` + `r round(cov(sampledata$X, sampledata$Y) / var(sampledata$X, na.rm = T), 2)`\cdot x$$
</div>

---
class: top, left
### Einfache lineare Regression

#### Lineare Regressionsfunktion

.pull-left[
```{r echo = F, out.height = "420px"}
scatterplot = ggplot(data = sampledata_long, aes(x = X, y = Y, label = ID, colour = Predicted)) +
  geom_segment(x = 0, xend = 10, y = coef(model)[["(Intercept)"]], yend = coef(model)[["(Intercept)"]], colour = "black") +
  geom_segment(x = 10, xend = 10, y = coef(model)[["(Intercept)"]], yend = predict(model, newdata = data.frame(X=10)), colour = "black") +
  geom_point(x = 0, y = coef(model)[["(Intercept)"]], colour = my_green) +
  geom_point() +
  geom_abline(slope = coef(model)[["X"]], 
              intercept = coef(model)[["(Intercept)"]], colour = my_green) +
   theme_classic()+
  scale_color_manual(values = c("yi Beobachtungswert" = "black",
                                  "≈∑i gesch√§tzter Wert" = my_green)) +
  theme(rect = element_rect(fill = "transparent")) +
  labs(colour = "", x = "Lernaufwand (X)", y = "Klausurerfolg (Y)") +
  annotate(geom = "Text",  label = "a", x = 0, y = coef(model)[["(Intercept)"]] + 5, colour =  "red") +
  annotate(geom = "Text",  label = "b", x = 12, y = predict(model, newdata = data.frame(X=10)) - (predict(model, newdata = data.frame(X=10)) - coef(model)[["(Intercept)"]])/2, colour =  "red") +
 coord_cartesian(xlim = c(0,100), ylim = c(0,100)) #+
#theme(axis.text = element_blank(), axis.ticks = element_blank(), axis.line = element_blank(), axis.title = element_blank())
ggplotly(scatterplot) %>%
layout(legend = list(x = 100, y = 0.5))
```
]

.pull-right[
$$ \hat{y}=`r round(mean(sampledata$Y, na.rm = T) - cov(sampledata$X, sampledata$Y) / var(sampledata$X, na.rm = T) * mean(sampledata$X, na.rm = T),2)` + `r round(cov(sampledata$X, sampledata$Y) / var(sampledata$X, na.rm = T), 2)`\cdot x$$

* Es ergibt sich also der gesch√§tzte $Y$ Wert $(\hat{y})$ aus a plus b mal $x$

**Nochmal zur√ºck zu unserer Interpretation:**
  
- a ist also der Wert wo $X=0$ ist. Also hat jemand mit 0 auf der UV einen AV Wert von 29.

- Wenn wir nun um 1 Einheit $X$ nach rechts gehen (in welcher Einheit die UV auch immmer gemessen wird), nimmt $\hat{y}$ um b zu.
]

---
class: top, left
### Einfache lineare Regression

#### Lineare Regressionsfunktion

.pull-left[
```{r echo = F, out.height = "420px"}
scatterplot = ggplot(data = sampledata_long, aes(x = X, y = Y, label = ID, colour = Predicted)) +
  geom_segment(x = 0, xend = 10, y = coef(model)[["(Intercept)"]], yend = coef(model)[["(Intercept)"]], colour = "black") +
  geom_segment(x = 10, xend = 10, y = coef(model)[["(Intercept)"]], yend = predict(model, newdata = data.frame(X=10)), colour = "black") +
  geom_point(x = 0, y = coef(model)[["(Intercept)"]], colour = my_green) +
  geom_point() +
  geom_abline(slope = coef(model)[["X"]], 
              intercept = coef(model)[["(Intercept)"]], colour = my_green) +
   theme_classic()+
  scale_color_manual(values = c("yi Beobachtungswert" = "black",
                                  "≈∑i gesch√§tzter Wert" = my_green)) +
  theme(rect = element_rect(fill = "transparent")) +
  labs(colour = "", x = "Lernaufwand (X)", y = "Klausurerfolg (Y)") +
  annotate(geom = "Text",  label = "a", x = 0, y = coef(model)[["(Intercept)"]] + 5, colour =  "red") +
  annotate(geom = "Text",  label = "b", x = 12, y = predict(model, newdata = data.frame(X=10)) - (predict(model, newdata = data.frame(X=10)) - coef(model)[["(Intercept)"]])/2, colour =  "red") +
 coord_cartesian(xlim = c(0,100), ylim = c(0,100)) #+
#theme(axis.text = element_blank(), axis.ticks = element_blank(), axis.line = element_blank(), axis.title = element_blank())
ggplotly(scatterplot) %>%
layout(legend = list(x = 100, y = 0.5))
```
]

.pull-right[
$$ \hat{y}=`r round(mean(sampledata$Y, na.rm = T) - cov(sampledata$X, sampledata$Y) / var(sampledata$X, na.rm = T) * mean(sampledata$X, na.rm = T),2)` + `r round(cov(sampledata$X, sampledata$Y) / var(sampledata$X, na.rm = T), 2)`\cdot x$$

* Wir k√∂nnten nun ausrechnen, welchen $Y$ Wert eine Person nach x Einheiten der UV hat.

* Welchen Wert erhalten wir z.B. f√ºr $X=10$?

]

---
class: top, left
### Einfache lineare Regression

#### Modellpassung

.pull-left[
```{r echo = F, out.height = "420px"}
scatterplot = ggplot(data = sampledata_long[sampledata_long$Predicted == "yi Beobachtungswert",], aes(x = X, y = Y, label = ID, colour = Predicted)) +
  geom_point() +
  #geom_text(check_overlap = TRUE, vjust = 0, nudge_y = nudgnumber) +
  scale_color_manual(values = c("yi Beobachtungswert" = "black",
                                  "≈∑i gesch√§tzter Wert" = my_green)) +
  geom_abline(slope = coef(model)[["X"]], 
              intercept = coef(model)[["(Intercept)"]], colour = my_green) +
   theme_classic() +
 coord_cartesian(xlim = c(0,100), ylim = c(0,100)) +
  labs(colour = "", x = "Lernaufwand (X)", y = "Klausurerfolg (Y)") +
theme(rect = element_rect(fill = "transparent")) 
ggplotly(scatterplot) %>%
layout(legend = list(x = 100, y = 0.5))
```
]


.pull-right[
Nach Aufstellen des Modells:

* Abst√§nde zwischen Beobachtungswerten und Regressionsgerade unterschiedlich gro√ü

**Frage:**

* Wie gut passt unser Modell auf die Beobachtungswerte?

* Ma√ü zur Bestimmung der Passung:

$\rightarrow$ Das Bestimmheitsma√ü $(R^2)$

]

---
class: top, left
### Einfache lineare Regression

#### Modellpassung

.pull-left[
```{r echo = F, out.height = "420px"}
scatterplot = ggplot(data = sampledata_long[sampledata_long$Predicted == "yi Beobachtungswert",], aes(x = X, y = Y, colour = Predicted)) +
  geom_hline(yintercept = mean(sampledata_long$Y[sampledata_long$Predicted == "yi Beobachtungswert"], na.rm=T)) +
  scale_y_continuous(breaks =  mean(sampledata_long$Y[sampledata_long$Predicted == "yi Beobachtungswert"], na.rm=T), labels = c("yÃÑ")) +
  geom_point() +
  scale_color_manual(values = c("yi Beobachtungswert" = "black",
                                  "≈∑i gesch√§tzter Wert" = my_green)) +
  # geom_abline(slope = coef(model)[["X"]], 
  #             intercept = coef(model)[["(Intercept)"]], colour = my_green) +
   theme_classic() +
 coord_cartesian(xlim = c(0,100), ylim = c(0,100)) +
   labs(colour = "", x = "Lernaufwand (X)", y = "Klausurerfolg (Y)") +
theme(rect = element_rect(fill = "transparent")) 
ggplotly(scatterplot) %>%
layout(legend = list(x = 100, y = 0.5))
```
]


.pull-right[
**Frage:**

* Wie gut passt unser Modell auf die Beobachtungswerte?

* horizontale Gerade = Mittelwert von $Y$ (um welchen Werte streuen)

$\rightarrow$ **Gesamtvarianz**

Regressionsgerade kann einen Anteil der Streuung um den Mittelwert erkl√§ren:

$\rightarrow$ **Aufgekl√§rte Varianz**

]

---
class: top, left
### Einfache lineare Regression

#### Modellpassung


```{r echo = F, out.height = "420px", out.width = "700px"}
scatterplot = ggplot(data = sampledata_long[sampledata_long$Predicted == "yi Beobachtungswert",], aes(x = X, y = Y, colour = Predicted)) +
  geom_hline(yintercept = mean(sampledata_long$Y[sampledata_long$Predicted == "yi Beobachtungswert"], na.rm=T)) +
  scale_y_continuous(breaks =  mean(sampledata_long$Y[sampledata_long$Predicted == "yi Beobachtungswert"], na.rm=T), labels = c("yÃÑ")) +
  geom_segment(aes(x = 12, 
                   xend = 12,
                   y=23,
                   yend= mean(sampledata_long$Y[sampledata_long$Predicted == "yi Beobachtungswert"], na.rm=T)), linetype = "dotted", colour = "red") +
  # geom_segment(aes(x = 12, 
  #                  xend = 12,
  #                  y=sampledata_long$Y[sampledata_long$X == 12 & sampledata_long$Predicted == "≈∑i gesch√§tzter Wert"],
  #                  yend= mean(sampledata_long$Y[sampledata_long$Predicted == "yi Beobachtungswert"], na.rm=T)), linetype = "dotted", colour = "green") +
  geom_point() +
  annotate(geom = "Text",  label = "Gesamt-\nvarianz", x = 12-nudgnumber - 5, y = mean(sampledata_long$Y[sampledata_long$Predicted == "yi Beobachtungswert"], na.rm=T) - sampledata_long$Y[sampledata_long$X == 12 & sampledata_long$Predicted == "≈∑i gesch√§tzter Wert"]/2, colour = "red") +
  scale_color_manual(values = c("yi Beobachtungswert" = "black",
                                  "≈∑i gesch√§tzter Wert" = my_green)) +
 # geom_abline(slope = coef(model)[["X"]], 
  #             intercept = coef(model)[["(Intercept)"]], colour = my_green) +
   theme_classic() +
 coord_cartesian(xlim = c(0,100), ylim = c(0,100)) +
   labs(colour = "", x = "Lernaufwand (X)", y = "Klausurerfolg (Y)") +
theme(rect = element_rect(fill = "transparent")) 
ggplotly(scatterplot) %>%
layout(legend = list(x = 100, y = 0.5))
```


---
class: top, left
### Einfache lineare Regression

#### Modellpassung

```{r echo = F, out.height = "420px", out.width = "700px"}
scatterplot = ggplot(data = sampledata_long, aes(x = X, y = Y, colour = Predicted)) +
  geom_hline(yintercept = mean(sampledata_long$Y[sampledata_long$Predicted == "yi Beobachtungswert"], na.rm=T)) +
  scale_y_continuous(breaks =  mean(sampledata_long$Y[sampledata_long$Predicted == "yi Beobachtungswert"], na.rm=T), labels = c("yÃÑ")) +
  geom_segment(aes(x = 12, 
                   xend = 12,
                   y=23,
                   yend= mean(sampledata_long$Y[sampledata_long$Predicted == "yi Beobachtungswert"], na.rm=T)), linetype = "dotted", colour = "red") +
  geom_segment(aes(x = 12, 
                   xend = 12,
                   y=sampledata_long$Y[sampledata_long$X == 12 & sampledata_long$Predicted == "≈∑i gesch√§tzter Wert"],
                   yend= mean(sampledata_long$Y[sampledata_long$Predicted == "yi Beobachtungswert"], na.rm=T)), linetype = "dotted", colour = "green") +
  geom_point() +
  annotate(geom = "Text",  label = "Aufgekl.\nVarianz", x = 12-nudgnumber - 5, y = sampledata_long$Y[sampledata_long$X == 12 & sampledata_long$Predicted == "≈∑i gesch√§tzter Wert"] + (mean(sampledata_long$Y[sampledata_long$Predicted == "yi Beobachtungswert"], na.rm=T) - sampledata_long$Y[sampledata_long$X == 12 & sampledata_long$Predicted == "≈∑i gesch√§tzter Wert"])/2, colour = "green") +
  #annotate(geom = "Text",  label = "Gesamt-\nvarianz", x = 12-nudgnumber-10, y = mean(sampledata_long$Y[sampledata_long$Predicted == "yi Beobachtungswert"], na.rm=T) - sampledata_long$Y[sampledata_long$X == 12 & sampledata_long$Predicted == "≈∑i gesch√§tzter Wert"]/2, colour = "red") +
  scale_color_manual(values = c("yi Beobachtungswert" = "black",
                                  "≈∑i gesch√§tzter Wert" = my_green)) +
  geom_abline(slope = coef(model)[["X"]], 
               intercept = coef(model)[["(Intercept)"]], colour = my_green) +
   theme_classic() +
 coord_cartesian(xlim = c(0,100), ylim = c(0,100)) +
  labs(colour = "", x = "Lernaufwand (X)", y = "Klausurerfolg (Y)") +
theme(rect = element_rect(fill = "transparent")) 
ggplotly(scatterplot) %>%
layout(legend = list(x = 100, y = 0.5))
```

---
class: top, left
### Einfache lineare Regression

#### Modellpassung

Das Verh√§ltnis aufgekl√§rter zu gesamter Streuung nennt sich Bestimmtheitsma√ü $(R^2)$

$$R^2=\frac{\text{erkl√§rte Varianz}}{\text{gesamte Streuung}}=\frac{\sum\limits _{i=1}^{n}(\hat{y}_i-\bar{y}_i)^2}{\sum\limits _{i=1}^{n}(y_i-\bar{y}_i)^2}=(\frac{s_{XY}}{s_X \cdot s_Y})^2$$
<br>

.center[
[**LINK zu interaktivem Regressionsbeispiel**](https://daze02-stephan-goerigk.shinyapps.io/Regression/)
]

---
class: top, left
### Einfache lineare Regression

#### Modellpassung

* $0‚â§ùëÖ^2‚â§1$
* Je n√§her $R^2$ an 1, desto besser passt sich Modell an Beobachtungspunkte an


```{r echo = F}
knitr::kable((t(sampledata[,2:3])),
             booktabs = T,
             longtable = F) %>%
  kable_classic(full_width = T,
                  font_size = 18,
                  html_font = "Times New Roman")
```

<br>

$$ R^2=(\frac{s_{xy}}{s_x\cdot s_y})^2 $$

$$ R^2=(\frac{`r round(cov(sampledata$X, sampledata$Y), 2)`}{`r round(sd(sampledata$X), 2)`\cdot `r round(sd(sampledata$Y), 2)`})^2 = `r round(cov(sampledata$X, sampledata$Y) / (sd(sampledata$X) * sd(sampledata$Y)), 2)`$$

Es k√∂nnen `r round(cov(sampledata$X, sampledata$Y) / (sd(sampledata$X) * sd(sampledata$Y)), 2) * 100`% der Streuung um den Mittelwert von Y durch die Gerade erkl√§rt werden.

---
class: top, left
### Einfache lineare Regression

#### Einsatz der Regression

Wozu k√∂nnen wir die Regression nutzen?

1. Als **Hypothesentest** f√ºr eine wissenschaftliche Hypothese (Inferenz):

	* Schritt 1: Mittels Regression Assoziation in der Stichprobe identifizieren
	* Schritt 2: Mittels Signifikanztest pr√ºfen, ob Assoziation wahrscheinlich auch au√üerhalb Stichprobe vorliegt 

2. Als **Vorhersagemodell** f√ºr neue Datenpunkte (Pr√§diktion):
	
	* Schritt 1: Mittels Stichprobendaten Regressionsmodell anpassen (X und Y bekannt)
	* Schritt 2: Mittels Modell neue Werte vorhersagen (X bekannt, Y unbekannt).

---
class: top, left
### Einfache lineare Regression

#### Berechnen der Regression in R

.code80[
```{r}
model = lm(Y ~ X, data = sampledata) # Aufstellen des Modells

summary(model) # Anzeigen des Modelloutputs
```
]

---
class: top, left
### Einfache lineare Regression

#### Berechnen der Regression in R

.pull-left[
.code80[
```{r}
model = lm(Y ~ X, data = sampledata) # Aufstellen des Modells

summary(model) # Anzeigen des Modelloutputs
```
]
]

.pull-right[
* Regressionskoeffizienten (a und b) stehen in der Spalte "Estimate"

* F√ºr jeden Koeffizienten wird ein spezieller t-Test (Wald-Test) gerechnet

  * $H_0$  a und b = 0
  * $H_1$  a und b $\neq$ 0
]
---
class: top, left
### Take-aways

.full-width[.content-box-gray[
* Zusammenh√§nge k√∂nnen neben Kovarianz/Korrelation auch mit der **Regression** quantifiziert werden.

* Regression ist sinnvoll, wenn aus den X-Werten auf die dazugeh√∂rigen Y-Werte **geschlossen** (diese vorhergesagt) werden soll.

* **Regressionsgerade** = graphische Veranschaulichung der Regressionsgleichung

* Regressionsgleichung ist definiert durch die **Regressionskoeffizienten** (Y-Achsenabschnitt und Steigung), welche aus Daten gesch√§tzt werden m√ºssen.

* **Y-Achsenabschnitt** ist der Startwert (wenn X = 0) und **Steigung** ist die Ver√§nderung in der AV bei Zunahme der UV um 1 Einheit.

* Das **Bestimmtheitsma√ü** $(R^2)$ gibt an, wie viel Prozent (%) der Gesamtvarianz der AV durch die UV (also durch die das Regressionsmodell) aufgekl√§rt werden.
]

]


